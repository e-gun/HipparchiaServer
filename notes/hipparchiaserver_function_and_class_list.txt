hipparchia_venv/ $ cloc --not-match-f='^j' --not-match-d='^golang' HipparchiaServer/
    199 text files.
    197 unique files.
     27 files ignored.

github.com/AlDanial/cloc v 1.88  T=0.37 s (491.2 files/s, 133519.5 lines/s)
-------------------------------------------------------------------------------
Language                     files          blank        comment           code
-------------------------------------------------------------------------------
Python                         121           7497          10289          17633
JavaScript                      18           1137            392           4232
CSS                              4            954            242           3914
HTML                            31            192             35           1954
XML                              7              0              0            712
Markdown                         1             63              0            225
-------------------------------------------------------------------------------
SUM:                           182           9843          10958          28670
-------------------------------------------------------------------------------


server/ $ grep "^class" */*.py && grep "^def " *.py && grep "^def " */*.py
dbsupport/redisdbfunctions.py:class NullRedis(object):
dbsupport/redisdbfunctions.py:class PooledRedisBorg(object):
hipparchiaobjects/authenticationobjects.py:class PassUser(object):
hipparchiaobjects/browserobjects.py:class BrowserOutputObject(object):
hipparchiaobjects/browserobjects.py:class BrowserPassageObject(object):
hipparchiaobjects/connectionobject.py:class GenericConnectionObject(object):
hipparchiaobjects/connectionobject.py:class PooledConnectionObject(GenericConnectionObject):
hipparchiaobjects/connectionobject.py:class SimpleConnectionObject(GenericConnectionObject):
hipparchiaobjects/cssformattingobject.py:class CssFormattingObject(object):
hipparchiaobjects/dbtextobjects.py:class dbAuthor(object):
hipparchiaobjects/dbtextobjects.py:class dbOpus(object):
hipparchiaobjects/dbtextobjects.py:class dbMorphologyObject(object):
hipparchiaobjects/dbtextobjects.py:class dbLemmaObject(object):
hipparchiaobjects/helperobjects.py:class LowandHighInfo(object):
hipparchiaobjects/helperobjects.py:class MPCounter(object):
hipparchiaobjects/helperobjects.py:class QueryCombinator(object):
hipparchiaobjects/helperobjects.py:class LSIVectorCorpus(object):
hipparchiaobjects/helperobjects.py:class LogEntropyVectorCorpus(object):
hipparchiaobjects/lexicalobjects.py:class dbDictionaryEntry(object):
hipparchiaobjects/lexicalobjects.py:class dbGreekWord(dbDictionaryEntry):
hipparchiaobjects/lexicalobjects.py:class dbLatinWord(dbDictionaryEntry):
hipparchiaobjects/lexicalobjects.py:class dbColumnorderGreekword(dbGreekWord):
hipparchiaobjects/lexicalobjects.py:class dbColumnorderLatinword(dbLatinWord):
hipparchiaobjects/lexicaloutputobjects.py:class multipleWordOutputObject(object):
hipparchiaobjects/lexicaloutputobjects.py:class lexicalOutputObject(object):
hipparchiaobjects/morphanalysisobjects.py:class MorphAnalysis(object):
hipparchiaobjects/morphanalysisobjects.py:class BaseFormMorphology(object):
hipparchiaobjects/morphanalysisobjects.py:class ConjugatedFormAnalysis(object):
hipparchiaobjects/morphanalysisobjects.py:class DeclinedFormAnalysis(object):
hipparchiaobjects/morphanalysisobjects.py:class AdvAnalysis(object):
hipparchiaobjects/morphanalysisobjects.py:class IndeclAnalysis(object):
hipparchiaobjects/morphanalysisobjects.py:class PrinciplePartFunctions(object):
hipparchiaobjects/morphanalysisobjects.py:class DeclinedFormFunctions(object):
hipparchiaobjects/morphanalysisobjects.py:class ConjugatedFormFunctions(object):
hipparchiaobjects/morphologyobjects.py:class MorphPossibilityObject(object):
hipparchiaobjects/parsingobjects.py:class InputParsingObject(object):
hipparchiaobjects/parsingobjects.py:class TextmakerInputParsingObject(InputParsingObject):
hipparchiaobjects/parsingobjects.py:class IndexmakerInputParsingObject(InputParsingObject):
hipparchiaobjects/parsingobjects.py:class BrowserInputParsingObject(InputParsingObject):
hipparchiaobjects/parsingobjects.py:class StructureInputParsingObject(InputParsingObject):
hipparchiaobjects/progresspoll.py:class SharedMemoryProgressPoll(object):
hipparchiaobjects/progresspoll.py:class RedisProgressPoll(object):
hipparchiaobjects/progresspoll.py:class NullProgressPoll(object):
hipparchiaobjects/searchfunctionobjects.py:class GenericSearchFunctionObject(object):
hipparchiaobjects/searchfunctionobjects.py:class RedisSearchFunctionObject(GenericSearchFunctionObject):
hipparchiaobjects/searchfunctionobjects.py:class ManagedListSearchFunctionObject(GenericSearchFunctionObject):
hipparchiaobjects/searchfunctionobjects.py:class QueuedSearchFunctionObject(GenericSearchFunctionObject):
hipparchiaobjects/searchobjects.py:class SearchObject(object):
hipparchiaobjects/searchobjects.py:class SearchOutputObject(object):
hipparchiaobjects/searchobjects.py:class SearchResult(object):
hipparchiaobjects/vectorobjects.py:class VectorValues(object):
hipparchiaobjects/wordcountobjects.py:class dbWordCountObject(object):
hipparchiaobjects/wordcountobjects.py:class dbHeadwordObject(dbWordCountObject):
hipparchiaobjects/worklineobject.py:class dbWorkLine(object):
calculatewordweights.py:def findtemporalweights(language: str) -> dict:
calculatewordweights.py:def findccorporaweights() -> dict:
calculatewordweights.py:def workobjectgeneraweights(language: str, iscollapsed: bool, workobjects) -> dict:
calculatewordweights.py:def findgeneraweights(language: str, collapsed=False) -> dict:
calculatewordweights.py:def findchronologicalweights(era: str, language: str) -> int:
calculatewordweights.py:def findgenreweightfromworkobject(genre: str, language: str, workdict: dict) -> int:
calculatewordweights.py:def findcorpusweight(corpus: str, language: str) -> int:
calculatewordweights.py:def findcorpusweightviawordcounts(corpus: str) -> int:
commandlineoptions.py:def getcommandlineargs():
configureatstartup.py:def startupprint(message: str, color='white', isbold=False, colorcoded=False, baremessage=True):
authentication/authenticationwrapper.py:def requireauthentication(routefunction):
authentication/authenticationwrapper.py:def loginfailurenotification() -> str:
authentication/knownusers.py:def loadusersdict(knownusersandpasswords=None):
browsing/browserfunctions.py:def buildbrowseroutputobject(authorobject: dbAuthor, workobject: dbOpus, locusindexvalue: int, dbcursor) -> BrowserOutputObject:
browsing/browserfunctions.py:def checkfordocumentmetadata(workline: dbWorkLine, workobject: dbOpus) -> str:
browsing/browserfunctions.py:def fetchhtmltemplateformetadatarow(shownotes=True):
browsing/browserfunctions.py:def browserfindlinenumberfromcitation(method: str, citationlist: list, workobject: dbOpus, dbcursor) -> tuple:
browsing/browserfunctions.py:def browserfindlinenumberfromlinenumber(citationlist: list, workobject: dbOpus, resultmessage: str, dbcursor) -> tuple:
browsing/browserfunctions.py:def browserfindlinenumberfromlocus(citationlist: list, workobject: dbOpus, resultmessage: str, dbcursor) -> tuple:
browsing/browserfunctions.py:def browserfindlinenumberfromperseus(citationlist: list, workobject: dbOpus, resultmessage: str, dbcursor) -> tuple:
dbsupport/bulkdboperations.py:def loadallauthorsasobjects() -> dict:
dbsupport/bulkdboperations.py:def loadallworksasobjects() -> dict:
dbsupport/bulkdboperations.py:def loadlemmataasobjects() -> dict:
dbsupport/bulkdboperations.py:def loadallworksintoallauthors(authorsdict, worksdict) -> dict:
dbsupport/bulkdboperations.py:def bulklexicalgrab(listofwords: List[str], tabletouse: str, targetcolumn: str, language: str) -> list:
dbsupport/citationfunctions.py:def findvalidlevelvalues(workobject: dbOpus, partialcitationtuple: tuple, cursor) -> LowandHighInfo:
dbsupport/citationfunctions.py:def locusintocitation(workobject: dbOpus, lineobject: dbWorkLine) -> str:
dbsupport/citationfunctions.py:def prolixlocus(workobject: dbOpus, citationtuple: tuple) -> str:
dbsupport/citationfunctions.py:def finddblinefromlocus(workobject: dbOpus, citationtuple: tuple, dbcursor, findlastline=False) -> int:
dbsupport/citationfunctions.py:def finddblinefromincompletelocus(workobject: dbOpus, citationlist: list, dbcursor, trialnumber=0, findlastline=False) -> dict:
dbsupport/citationfunctions.py:def perseuslookupleveltrimmer(workobject: dbOpus, citationlist: list, cursor, trialnumber: int) -> dict:
dbsupport/citationfunctions.py:def perseuslookupchangecase(citationlist: list) -> list:
dbsupport/citationfunctions.py:def perseusdelabeler(citationlist: list, workobject: dbOpus) -> list:
dbsupport/citationfunctions.py:def perseuscitationsintohipparchiacitations(citationlist: list) -> list:
dbsupport/dbbuildinfo.py:def buildoptionchecking() -> dict:
dbsupport/dbbuildinfo.py:def versionchecking(activedbs: list, expectedsqltemplateversion: str) -> str:
dbsupport/dblinefunctions.py:def dblineintolineobject(dbline: tuple) -> dbWorkLine:
dbsupport/dblinefunctions.py:def grabonelinefromwork(workdbname: str, lineindex: int, cursor) -> tuple:
dbsupport/dblinefunctions.py:def returnfirstorlastlinenumber(workid: str, dbcursor, findlastline=False, disallowt=False, disallowlevel=0) -> int:
dbsupport/dblinefunctions.py:def makeablankline(work: str, fakelinenumber: int) -> dbWorkLine:
dbsupport/dblinefunctions.py:def bulklinegrabber(table: str, column: str, criterion: str, setofcriteria, cursor) -> dict:
dbsupport/dblinefunctions.py:def grablistoflines(table: str, uidlist: list, dbcursor=None) -> list:
dbsupport/dblinefunctions.py:def grabbundlesoflines(worksandboundaries: dict, cursor) -> list:
dbsupport/dblinefunctions.py:def bulkenvironsfetcher(table: str, searchresultlist: list, context: int) -> list:
dbsupport/lexicaldbfunctions.py:def headwordsearch(seeking: str, limit: str, usedictionary: str, usecolumn: str) -> List[tuple]:
dbsupport/lexicaldbfunctions.py:def reversedictionarylookup(seeking: str, usedict: str, limit=None) -> List:
dbsupport/lexicaldbfunctions.py:def findentrybyid(usedict: str, entryid: str) -> dbDictionaryEntry:
dbsupport/lexicaldbfunctions.py:def querytotalwordcounts(word: str, dbcursor=None) -> dbHeadwordObject:
dbsupport/lexicaldbfunctions.py:def probedictionary(usedictionary: str, usecolumn: str, seeking: str, syntax: str, dbcursor=None, trialnumber=0) -> List:
dbsupport/lexicaldbfunctions.py:def convertdictionaryfindintowordobject(foundline: tuple, usedictionary: str, dbcursor):
dbsupport/lexicaldbfunctions.py:def findcountsviawordcountstable(wordtocheck):
dbsupport/lexicaldbfunctions.py:def grablemmataobjectfor(db, dbcursor=None, word=None, xref=None, allowsuperscripts=False):
dbsupport/lexicaldbfunctions.py:def findparserxref(wordobject) -> str:
dbsupport/lexicaldbfunctions.py:def lookformorphologymatches(word: str, dbcursor, trialnumber=0, revertword=None, rewrite=None, furtherdeabbreviate=False) -> dbMorphologyObject:
dbsupport/lexicaldbfunctions.py:def bulkfindwordcounts(listofwords: List[str]) -> List[dbWordCountObject]:
dbsupport/lexicaldbfunctions.py:def bulkfindmorphologyobjects(listofwords: List[str], language: str) -> List[dbMorphologyObject]:
dbsupport/miscdbfunctions.py:def resultiterator(cursor, chunksize=5000):
dbsupport/miscdbfunctions.py:def dbloadasingleworkobject(workuniversalid: str) -> dbOpus:
dbsupport/miscdbfunctions.py:def findselectionboundaries(workobject: dbOpus, selection: str, cursor) -> tuple:
dbsupport/miscdbfunctions.py:def simplecontextgrabber(authortable: str, focusline: int, linesofcontext: int, cursor) -> list:
dbsupport/miscdbfunctions.py:def perseusidmismatch(badworkdbnumber: str, cursor) -> str:
dbsupport/miscdbfunctions.py:def returnfirstwork(authorid: str, dbcursor=None) -> str:
dbsupport/miscdbfunctions.py:def makeanemptyauthor(universalid: str) -> dbAuthor:
dbsupport/miscdbfunctions.py:def makeanemptywork(universalid: str) -> dbOpus:
dbsupport/miscdbfunctions.py:def getpostgresserverversion() -> str:
dbsupport/miscdbfunctions.py:def probefordatabases() -> dict:
dbsupport/miscdbfunctions.py:def icanpickleconnections(dothecheck=False) -> bool:
dbsupport/miscdbfunctions.py:def cleanpoolifneeded():
dbsupport/redisdbfunctions.py:def establishredisconnection() -> redis.client.Redis:
dbsupport/redisdbfunctions.py:def buildredissearchlist(listofsearchlocations: List, searchid: str):
dbsupport/redisdbfunctions.py:def loadredisresults(searchid):
dbsupport/tablefunctions.py:def assignuniquename(maxlength=None) -> str:
dbsupport/vectordbfunctions.py:def createvectorstable():
dbsupport/vectordbfunctions.py:def createstoredimagestable():
dbsupport/vectordbfunctions.py:def storevectorindatabase(searchobject: SearchObject, vectortype: str, vectorspace):
dbsupport/vectordbfunctions.py:def checkforstoredvector(searchobject: SearchObject, vectortype: str, careabout='thumbprint'):
dbsupport/vectordbfunctions.py:def fetchverctorenvirons(hitdict: dict, searchobject: SearchObject) -> list:
experimental/gensimexperiments.py:def gensimexperiment(so):
experimental/gensimexperiments.py:def doc2vecbuildspace(morphdict, sentences):
experimental/gensimexperiments.py:def logentropybuildspace(searchobject, morphdict, sentences):
experimental/graphingexperiments.py:def bokehgraphmatches(graphtitle, searchterm, mostsimilartuples, terms, relevantconnections, vtype):
experimental/scikitlearnvectors.py:def fortestingpurposessklearnselectedworks(searchobject):
experimental/scikitlearnvectors.py:def generatesimilarsentenceoutput(corehtml, searchobject, workssearched, matches):
experimental/scikitlearnvectors.py:def sklearntextfeatureextractionandevaluation(sentences, searchobject):
experimental/scikitlearnvectors.py:def simplesktextcomparison(sentencetuples, searchobject, stopwordsbyheadword=False):
experimental/scikitlearnvectors.py:def print_top_words(model, feature_names, n_top_words):
experimental/scikitlearnvectors.py:def ldatopicmodeling(sentencetuples, searchobject):
experimental/tensorflowvectors.py:def tensorgraphelectedworks(searchobject):
experimental/tensorflowvectors.py:def tftrainondata(sentences, searchobject):
experimental/tensorflowvectors.py:def builddatasetdict(words, vocabularysize):
experimental/tensorflowvectors.py:def tfgeneratetrainingbatch(batchsize, numberofskips, skipwindow, thedata, dataindex):
experimental/tensorflowvectors.py:def tfplotwithlabels(low_dim_embs, labels, filename):
experimental/tensorflowvectors.py:def tfnlptraining(sentences, searchobject):
experimental/tensorflowvectors.py:def converttexttoinexvalues(sentences, morphdict):
experimental/tensorflowvectors.py:def generatetfbatch(sentences, batchsize, windowsize, method='cbow'):
experimental/tensorflowvectors.py:def tfnlpwork(textasvals, wordsmappedtocodes, codesmappedtowords, activepoll):
experimental/threedimensionalplots.py:def log(msg):
experimental/threedimensionalplots.py:def sampleVectors(vectors, percentagetouse):
experimental/threedimensionalplots.py:def reduceWithPCA(vectors, size):
experimental/threedimensionalplots.py:def reduceWithUMAP(vectors, size):
experimental/threedimensionalplots.py:def reduceWithTSNE(vectors, size):
experimental/threedimensionalplots.py:def PCA_then_UMAP(vectors, pca_size, umap_size):
experimental/threedimensionalplots.py:def PCA_then_TSNE(vectors, pca_size, tsne_size):
experimental/threedimensionalplots.py:def clusterForColour(vectors, size):
experimental/threedimensionalplots.py:def saveAsGraphitFile(model, vectors, indices, clusters, fname):
experimental/threedimensionalplots.py:def plot2D(vectors):
formatting/abbreviations.py:def deabbreviateauthors(authorabbr: str, lang: str) -> str:
formatting/abbreviations.py:def deabrevviategreekauthors() -> Dict[str, str]:
formatting/abbreviations.py:def deabrevviatelatinauthors() -> Dict[str, str]:
formatting/abbreviations.py:def unpackcommonabbreviations(potentialabbreviaiton: str, furtherunpack: bool) -> str:
formatting/betacodeescapes.py:def andsubstitutes(match: re.match) -> str:
formatting/betacodetounicode.py:def replacegreekbetacode(texttoclean: str) -> str:
formatting/betacodetounicode.py:def capitalletters(betacode: str) -> str:
formatting/betacodetounicode.py:def capitalsmoothgraveadscript(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def capitalroughgraveadscript(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def capitalsmoothacuteadscript(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def capitalroughacuteadscript(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def capitalsmoothcircumflexadscript(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def capitalroughcircumflexadscript(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def capitalsmoothgrave(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def capitalroughgrave(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def capitalsmoothacute(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def capitalroughacute(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def capitalsmoothcircumflex(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def capitalroughcircumflex(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def capitalsmooth(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def capitalrough(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def capitalgrave(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def capitalacute(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def capitaladscript(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def capitals(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def lowercaseletters(betacode: str) -> str:
formatting/betacodetounicode.py:def lowercasesmoothgravesubscript(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def lowercaseroughgravesubscript(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def lowercasesmoothacutesubscript(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def lowercaseroughacutesubscript(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def lowercasesmoothcircumflexsubscript(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def lowercaseroughcircumflexsubscript(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def lowercasesmoothgrave(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def lowercaseroughgrave(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def lowercasesmoothacute(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def lowercaseroughacute(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def lowercasesmoothcircumflex(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def lowercaseroughcircumflex(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def lowercasegravesub(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def lowercaseacutedsub(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def lowercasesircumflexsub(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def lowercasesmoothsub(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def lowercaseroughsub(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def lowercasegravediaresis(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def lowercaseacutediaresis(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def lowercasesircumflexdiaresis(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def lowercasesmooth(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def lowercaserough(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def lowercasegrave(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def lowercaseacute(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def lowercascircumflex(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def lowercasediaresis(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def lowercasesubscript(match: re.match, g=1) -> str:
formatting/betacodetounicode.py:def lowercases(match: re.match, g=0) -> str:
formatting/bibliographicformatting.py:def bcedating(s=None) -> tuple:
formatting/bibliographicformatting.py:def formatauthinfo(authorobject: dbAuthor) -> str:
formatting/bibliographicformatting.py:def woformatworkinfo(workobject: dbOpus) -> str:
formatting/bibliographicformatting.py:def formatname(workobject: dbOpus, authorobject: dbAuthor) -> str:
formatting/bibliographicformatting.py:def formatpublicationinfo(pubinfo: str) -> str:
formatting/bibliographicformatting.py:def avoidlonglines(string: str, maxlen: int, splitval: str, stringlist=None) -> str:
formatting/bibliographicformatting.py:def formatauthorandworkinfo(authorname: str, workobject: dbOpus, countprovided=False) -> str:
formatting/bracketformatting.py:def gtltsubstitutes(text: str) -> str:
formatting/bracketformatting.py:def brackethtmlifysearchfinds(listoflineobjects: list, searchobject: SearchObject, linehtmltemplate: str) -> list:
formatting/browserformatting.py:def insertparserids(lineobject: dbWorkLine, continuationdict: dict) -> str:
formatting/browserformatting.py:def bracketcheck(word: str) -> bool:
formatting/browserformatting.py:def addobservedtags(word: str, lastword: str, hyphenated: str) -> str:
formatting/frontpagehtmlformatting.py:def vectorhtmlforfrontpage() -> str:
formatting/frontpagehtmlformatting.py:def vectorhtmlforoptionsbar() -> str:
formatting/frontpagehtmlformatting.py:def getsearchfieldbuttonshtml() -> str:
formatting/frontpagehtmlformatting.py:def getauthorholdingfieldhtml() -> str:
formatting/frontpagehtmlformatting.py:def getdaterangefieldhtml() -> str:
formatting/frontpagehtmlformatting.py:def getlexicafieldhtml() -> str:
formatting/jsformatting.py:def insertbrowserclickjs(tagname: str) -> str:
formatting/jsformatting.py:def insertlexicalbrowserjs() -> str:
formatting/jsformatting.py:def generatevectorjs() -> str:
formatting/jsformatting.py:def supplementalvocablistjs() -> str:
formatting/jsformatting.py:def supplementalindexjs() -> str:
formatting/jsformatting.py:def observedformjs(clickone, clicktwo) -> str:
formatting/jsformatting.py:def dictionaryentryjs() -> str:
formatting/jsformatting.py:def morphologychartjs() -> str:
formatting/lexicaformatting.py:def formatdictionarysummary(wordentryobject) -> str:
formatting/lexicaformatting.py:def formatparsinginformation(possibilitieslist: List[MorphPossibilityObject]) -> str:
formatting/lexicaformatting.py:def getobservedwordprevalencedata(dictionaryword) -> str:
formatting/lexicaformatting.py:def formatprevalencedata(wordcountobject) -> str:
formatting/miscformatting.py:def htmlcommentdecorator(function):
formatting/miscformatting.py:def timedecorator(function):
formatting/miscformatting.py:def validatepollid(searchid, maxchars=36) -> str:
formatting/miscformatting.py:def consolewarning(message: str, color='yellow', isbold=False, colorcoded=True, baremessage=False):
formatting/miscformatting.py:def debugmessage(message: str):
formatting/morphologytableformatting.py:def findmygreektenses(mood: str, voice: str) -> dict:
formatting/morphologytableformatting.py:def findmylatintenses(mood: str, voice: str) -> dict:
formatting/morphologytableformatting.py:def declinedtabletemplate(dialect='attic', duals=True, lang='greek', skipgenders=None) -> str:
formatting/morphologytableformatting.py:def verbtabletemplate(mood: str, voice: str, dialect='attic', duals=True, lang='greek') -> str:
formatting/morphologytableformatting.py:def filloutmorphtabletemplate(lookupdict: dict, wordcountdict: dict, template: str) -> str:
formatting/morphologytableformatting.py:def purgeemptyrows(templatetoclean: str) -> str:
formatting/searchformatting.py:def buildresultobjects(hitdict: dict, authordict: dict, workdict: dict, searchobject: SearchObject) -> ResultList:
formatting/searchformatting.py:def rewriteskgandprx(skg: str, prx: str, htmlsearch: str, so: SearchObject) -> dict:
formatting/searchformatting.py:def flagsearchterms(searchresultobject: SearchResult, skg: str, prx: str, searchobject: SearchObject) -> LineList:
formatting/searchformatting.py:def highlightsearchterm(lineobject: dbWorkLine, regexequivalent, spanname) -> str:
formatting/searchformatting.py:def htmlifysearchfinds(listofsearchresultobjects: ResultList, searchobject: SearchObject) -> str:
formatting/searchformatting.py:def nocontexthtmlifysearchfinds(listofsearchresultobjects: ResultList) -> str:
formatting/searchformatting.py:def unbalancedspancleaner(html: str) -> str:
formatting/sessionhtmlandjs.py:def sessionselectionsashtml(authordict: dict, workdict: dict) -> dict:
formatting/sessionhtmlandjs.py:def sessionselectionsjs(labeltupleslist: List[Tuple[str, int]]) -> str:
formatting/sessionhtmlandjs.py:def sessiontimeexclusionsinfo():
formatting/sessionhtmlandjs.py:def sessionselectionsinfo(authordict: dict, workdict: dict) -> dict:
formatting/sessionhtmlandjs.py:def selectionlinehtmlandjs(v: str, selectionorexclusion: str, thesession: session) -> dict:
formatting/vectorformatting.py:def formatlsimatches(listofmatches: List[dict]) -> str:
formatting/vectorformatting.py:def formatnnmatches(listofneighbors: List[tuple], vectorvalues: VectorValues):
formatting/vectorformatting.py:def formatnnsimilarity(termone: str, termtwo: str, similarityscore: float) -> str:
formatting/vectorformatting.py:def skformatmostimilar(similaritiesdict: dict) -> str:
formatting/vectorformatting.py:def locusformat(dblineobject: dbWorkLine) -> str:
formatting/vectorformatting.py:def nearestneighborgenerateoutput(findshtml: str, mostsimilar: list, imagename: str, workssearched: int, searchobject: SearchObject) -> str:
formatting/vectorformatting.py:def analogiesgenerateoutput(searchobject, findstuples: list) -> JSON_STR:
formatting/vectorformatting.py:def lsiformatoutput(findshtml: str, workssearched: int, matches: list, searchobject: SearchObject) -> str:
formatting/vectorformatting.py:def ldatopicsgenerateoutput(ldavishtmlandjs: str, workssearched: int, settings: dict, searchobject: SearchObject):
formatting/wordformatting.py:def removegravity(accentedword: str) -> str:
formatting/wordformatting.py:def forceterminalacute(matchgroup: re.match) -> str:
formatting/wordformatting.py:def stripaccents(texttostrip: str, transtable=None) -> str:
formatting/wordformatting.py:def buildhipparchiatranstable() -> dict:
formatting/wordformatting.py:def gkattemptelision(hypenatedgreekheadword: str) -> str:
formatting/wordformatting.py:def latattemptelision(hypenatedlatinheadword: str) -> str:
formatting/wordformatting.py:def tidyupterm(word: str, punct=None) -> str:
formatting/wordformatting.py:def universalregexequivalent(searchterm: str) -> str:
formatting/wordformatting.py:def depunct(stringtoclean: str, allowedpunctuationsting=None) -> str:
formatting/wordformatting.py:def avoidsmallvariants(text: str) -> str:
formatting/wordformatting.py:def forcelunates(text: str) -> str:
formatting/wordformatting.py:def basiclemmacleanup(text: str) -> str:
formatting/wordformatting.py:def attemptsigmadifferentiation(text: str) -> str:
formatting/wordformatting.py:def abbreviatedsigmarestoration(text: str) -> str:
formatting/wordformatting.py:def wordlistintoregex(wordlist: list) -> str:
formatting/wordformatting.py:def upperorlowerregex(word: str) -> str:
formatting/wordformatting.py:def acuteorgrav(word: str) -> str:
formatting/wordformatting.py:def setdictionarylanguage(thisword) -> str:
formatting/wordformatting.py:def uforvoutsideofmarkup(textwithmarkup) -> str:
formatting/wordformatting.py:def citationcharacterset() -> set:
formatting/wordformatting.py:def reducetovalidcitationcharacters(text: str, supplement=None) -> str:
hipparchiaobjects/searchfunctionobjects.py:def returnsearchfncobject(workerid, foundlineobjects, listofplacestosearch, searchobject, dbconnection, searchfunction):
listsandsession/checksession.py:def probeforsessionvariables():
listsandsession/checksession.py:def convertyesnototruefalse():
listsandsession/checksession.py:def corpusselectionsasavalue(thesession=None) -> int:
listsandsession/checksession.py:def corpusselectionsaspseudobinarystring(thesession=None) -> str:
listsandsession/checksession.py:def justlatin(thesession=None) -> bool:
listsandsession/checksession.py:def justtlg(thesession=None) -> bool:
listsandsession/checksession.py:def justinscriptions(thesession=None) -> bool:
listsandsession/checksession.py:def justpapyri(thesession=None) -> bool:
listsandsession/checksession.py:def justlit(thesession=None) -> bool:
listsandsession/checksession.py:def justdoc(thesession=None) -> bool:
listsandsession/checksession.py:def corpusisonandavailable(corpusname):
listsandsession/genericlistfunctions.py:def flattenlistoflists(listoflists):
listsandsession/genericlistfunctions.py:def dictitemstartswith(originaldict: dict, element: str, muststartwith: str) -> dict:
listsandsession/genericlistfunctions.py:def findspecificdate(authorandworklist, authorobjectdict, workobjectdict, specificdate) -> list:
listsandsession/genericlistfunctions.py:def tidyuplist(untidylist: list) -> list:
listsandsession/genericlistfunctions.py:def dropdupes(checklist: list, matchlist: list) -> list:
listsandsession/genericlistfunctions.py:def polytonicsort(unsortedwords: list) -> list:
listsandsession/genericlistfunctions.py:def foundindict(searchdict: dict, element: str, mustbein: str, exactmatch=True) -> list:
listsandsession/genericlistfunctions.py:def findsetofallwords(listofwordclusters: list) -> set:
listsandsession/searchlistmanagement.py:def compilesearchlist(listmapper: dict, s: dict) -> list:
listsandsession/searchlistmanagement.py:def sortsearchlist(searchlist: list, authorsdict: dict) -> list:
listsandsession/searchlistmanagement.py:def sortresultslist(hits: list, searchobject: SearchObject, authorsdict: dict, worksdict: dict) -> dict:
listsandsession/searchlistmanagement.py:def calculatewholeauthorsearches(searchlist: list, authordict: dict) -> list:
listsandsession/searchlistmanagement.py:def flagexclusions(searchlist: list, s=session) -> list:
listsandsession/searchlistmanagement.py:def prunebydate(searchlist: list, authorobjectdict: dict, workobjectdict: dict, s=session) -> list:
listsandsession/searchlistmanagement.py:def removespuria(searchlist: list, worksdict: dict) -> list:
listsandsession/sessiondicts.py:def buildaugenresdict(authordict: dict) -> dict:
listsandsession/sessiondicts.py:def buildworkgenresdict(workdict: dict) -> dict:
listsandsession/sessiondicts.py:def buildauthorlocationdict(authordict: dict) -> dict:
listsandsession/sessiondicts.py:def buildworkprovenancedict(workdict: dict) -> dict:
listsandsession/sessiondicts.py:def buildkeyedlemmata(listofentries: dict) -> defaultdict:
listsandsession/sessionfunctions.py:def modifysessionvariable(parameter, value):
listsandsession/sessionfunctions.py:def validatevectorvalue(parameter, value):
listsandsession/sessionfunctions.py:def modifysessionselections(cookiedict, authorgenreslist, workgenreslist, authorlocationlist, workprovenancelist):
listsandsession/sessionfunctions.py:def parsejscookie(cookiestring: str) -> dict:
listsandsession/sessionfunctions.py:def rationalizeselections(newselectionuid, selectorexclude):
listsandsession/sessionfunctions.py:def reducetosessionselections(listmapper: dict, criterion: str) -> dict:
listsandsession/sessionfunctions.py:def returnactivedbs(thesession=None) -> List[str]:
listsandsession/sessionfunctions.py:def findactivebrackethighlighting(thesession=None) -> List[str]:
listsandsession/sessionfunctions.py:def selectionisactive(selected) -> str:
listsandsession/sessionfunctions.py:def returnactivelist(selectiondict: dict) -> List[str]:
listsandsession/validateconfig.py:def loadconfig(filepath) -> list:
listsandsession/validateconfig.py:def parseconfig(configlist) -> List[str]:
listsandsession/validateconfig.py:def compareconfigs(template, model) -> Dict[str, set]:
listsandsession/whereclauses.py:def configurewhereclausedata(searchlist: list, workdict: dict, searchobject: SearchObject) -> Dict[str, dict]:
listsandsession/whereclauses.py:def wholeworkbetweenclausecontents(listofworkobjects: list) -> List[Tuple[int, int]]:
listsandsession/whereclauses.py:def wholeworktemptablecontents(authorid: str, setoflinenumbers: set) -> Dict[str, str]:
listsandsession/whereclauses.py:def partialworkbetweenclausecontents(workobject: dbOpus, searchobject: SearchObject) -> Tuple[str, Dict[str, list]]:
routes/authenticationroutes.py:def authenticationactions(action: str) -> JSON_OR_RESPONSE:
routes/authenticationroutes.py:def hipparchialogin() -> FlaskResponse:
routes/authenticationroutes.py:def hipparchialogout() -> JSON_STR:
routes/authenticationroutes.py:def checkuser() -> JSON_STR:
routes/browseroute.py:def grabtextforbrowsing(method, author, work, location=None) -> JSON_STR:
routes/browseroute.py:def rawcitationgrabtextforbrowsing(author: str, work: str, location=None) -> JSON_STR:
routes/cssroutes.py:def loadcssfile(cssrequest) -> FlaskResponse:
routes/debuggingroutes.py:def databasecontents(dictionarytodisplay) -> PAGE_STR:
routes/debuggingroutes.py:def styesheetsamples() -> PAGE_STR:
routes/debuggingroutes.py:def showsessioncontents() -> PAGE_STR:
routes/debuggingroutes.py:def testroute() -> PAGE_STR:
routes/frontpage.py:def frontpage() -> PAGE_STR:
routes/frontpage.py:def sendfavicon() -> FlaskResponse:
routes/frontpage.py:def appletouchticon() -> FlaskResponse:
routes/frontpage.py:def robotstxt() -> FlaskResponse:
routes/frontpage.py:def errorhandlingpage(errornumber) -> PAGE_STR:
routes/frontpage.py:def badrequesterror(e) -> PAGE_STR:
routes/frontpage.py:def pagenotfound(e) -> PAGE_STR:
routes/frontpage.py:def internalservererror(e) -> PAGE_STR:
routes/getterroutes.py:def responsegetter(fnc: str, param: str) -> FlaskResponse:
routes/getterroutes.py:def cookieintosession(cookienum) -> FlaskResponse:
routes/getterroutes.py:def fetchstoredimage(figurename) -> FlaskResponse:
routes/getterroutes.py:def infogetter(fnc: str, one=None, two=None, three=None) -> JSON_STR:
routes/getterroutes.py:def getsessionvariables() -> JSON_STR:
routes/getterroutes.py:def findtheworksof(authoruid) -> JSON_STR:
routes/getterroutes.py:def findworkstructure(author, work, passage=None) -> JSON_STR:
routes/getterroutes.py:def sampleworkcitation(authorid: str, workid: str) -> JSON_STR:
routes/getterroutes.py:def getauthinfo(authorid: str) -> JSON_STR:
routes/getterroutes.py:def getsearchlistcontents() -> JSON_STR:
routes/getterroutes.py:def getgenrelistcontents() -> JSON_STR:
routes/getterroutes.py:def returnvectorsettingsranges() -> JSON_STR:
routes/getterroutes.py:def loadhelpdata() -> JSON_STR:
routes/hintroutes.py:def supplyhints(category, _) -> JSON_STR:
routes/hintroutes.py:def offerauthorhints(query) -> list:
routes/hintroutes.py:def generichintlist(query, lookupdict, errortext):
routes/hintroutes.py:def augenrelist(query) -> list:
routes/hintroutes.py:def wkgenrelist(query) -> list:
routes/hintroutes.py:def offeraulocationhints(query) -> list:
routes/hintroutes.py:def offerprovenancehints(query) -> list:
routes/hintroutes.py:def buildhintlist(seeking: str, listofposiblities: list) -> list:
routes/hintroutes.py:def offerlemmatahints(query) -> list:
routes/lexicalroutes.py:def lexicalgetter(action: str, one=None, two=None, three=None, four=None) -> JSON_STR:
routes/lexicalroutes.py:def dictsearch(searchterm) -> JSON_STR:
routes/lexicalroutes.py:def findbyform(observedword, authorid=None) -> JSON_STR:
routes/lexicalroutes.py:def reverselexiconsearch(searchid, searchterm) -> JSON_STR:
routes/lexicalroutes.py:def dictionaryidsearch(language, entryid) -> JSON_STR:
routes/lexicalroutes.py:def knownforms(language, lexicalid, xrefid, headword) -> JSON_STR:
routes/resetroutes.py:def resetroute(item) -> FlaskResponse:
routes/searchroute.py:def searchgetter(action: str, one=None, two=None) -> JSON_STR:
routes/searchroute.py:def executesearch(searchid: str, so=None, req=request) -> JSON_STR:
routes/searchroute.py:def singlewordsearch(searchid, searchterm) -> JSON_STR:
routes/searchroute.py:def headwordsearch(searchid, headform) -> JSON_STR:
routes/searchroute.py:def checkforactivesearch(searchid, trialnumber=0) -> JSON_STR:
routes/searchroute.py:def externalwsgipolling(pollid) -> JSON_STR:
routes/selectionroutes.py:def selectionmaker(action: str, one=None, two=None) -> JSON_STR:
routes/selectionroutes.py:def selectionmade(requestargs: MultiDict) -> JSON_STR:
routes/selectionroutes.py:def clearselections(category, index=-1) -> JSON_STR:
routes/selectionroutes.py:def getcurrentselections() -> JSON_STR:
routes/selectionroutes.py:def setsessionvariable(thevariable, thevalue) -> JSON_STR:
routes/textandindexroutes.py:def textgetter(action: str, one=None, two=None, three=None, four=None, five=None) -> JSON_STR:
routes/textandindexroutes.py:def buildindexto(searchid: str, author: str, work=None, passage=None, endpoint=None, citationdelimiter='|', justvocab=False) -> JSON_STR:
routes/textandindexroutes.py:def generatevocabfor(searchid: str, author: str, work=None, passage=None, endpoint=None, citationdelimiter='|') -> JSON_STR:
routes/textandindexroutes.py:def vocabfromrawlocus(searchid: str, author: str, work=None, location=None, endpoint=None) -> JSON_STR:
routes/textandindexroutes.py:def indexfromrawlocus(searchid: str, author: str, work=None, location=None, endpoint=None) -> JSON_STR:
routes/textandindexroutes.py:def textmaker(author: str, work=None, passage=None, endpoint=None, citationdelimiter='|') -> JSON_STR:
routes/textandindexroutes.py:def texmakerfromrawlocus(author: str, work: str, location: str, endpoint=None) -> JSON_STR:
routes/vectorroutes.py:def dispatchvectorsearch(vectortype: str, searchid: str, one=None, two=None, three=None) -> JSON_STR:
routes/vectorroutes.py:def buildsinglelemmasearchobject(pollid: str, one: str) -> SearchObject:
routes/vectorroutes.py:def buildtriplelemmasearchobject(pollid, one, two, three) -> SearchObject:
searching/dynamicsqlsearchdispatching.py:def dynamicsqlsearchdispatcher(searchobject: SearchObject) -> List[dbWorkLine]:
searching/dynamicsqlsearchdispatching.py:def workonsimplesearch(workerid: int, foundlineobjects: ListProxy, listofplacestosearch: ListProxy, searchobject: SearchObject, dbconnection) -> ListProxy:
searching/dynamicsqlsearchdispatching.py:def workonproximitysearch(workerid: int, foundlineobjects: ListProxy, listofplacestosearch: ListProxy, searchobject: SearchObject, dbconnection) -> ListProxy:
searching/dynamicsqlsearchdispatching.py:def workonphrasesearch(workerid: int, foundlineobjects: ListProxy, listofplacestosearch: ListProxy, searchobject: SearchObject, dbconnection) -> ListProxy:
searching/dynamicsqlsearchdispatching.py:def workonsimplelemmasearch(workerid: int, foundlineobjects: ListProxy, searchtuples: ListProxy, searchobject: SearchObject, dbconnection) -> ListProxy:
searching/dynamicsqlsearching.py:def substringsearch(seeking: str, authortable: str, searchobject: SearchObject, cursor, templimit=None) -> Generator:
searching/dynamicsqlsearching.py:def withinxlines(workdbname: str, searchobject: SearchObject, dbconnection) -> List[tuple]:
searching/dynamicsqlsearching.py:def lemmatizedwithinxlines(searchobject: SearchObject, hitlist: List[tuple], dbcursor):
searching/dynamicsqlsearching.py:def simplewithinxlines(searchobject: SearchObject, hitlist: List[tuple], dbcursor) -> List[tuple]:
searching/dynamicsqlsearching.py:def withinxwords(workdbname: str, searchobject: SearchObject, dbconnection) -> List[dbWorkLine]:
searching/dynamicsqlsearching.py:def phrasesearch(wkid: str, searchobject: SearchObject, cursor) -> List[dbWorkLine]:
searching/dynamicsqlsearching.py:def subqueryphrasesearch(workerid, foundlineobjects: ListProxy, searchphrase: str, listofplacestosearch: ListProxy,
searching/precomposedsqlsearching.py:def precomposedsqlsearch(so: SearchObject) -> List[dbWorkLine]:
searching/precomposedsqlsearching.py:def basicprecomposedsqlsearcher(so: SearchObject) -> List[dbWorkLine]:
searching/precomposedsqlsearching.py:def precomposedgolangsearcher(so: SearchObject) -> List[dbWorkLine]:
searching/precomposedsqlsearching.py:def sharedlibraryclisearcher(so: SearchObject) -> str:
searching/precomposedsqlsearching.py:def sharedlibrarysearcher(so: SearchObject) -> str:
searching/precomposedsqlsearching.py:def generatepreliminaryhitlist(so: SearchObject, recap=hipparchia.config['INTERMEDIATESEARCHCAP']) -> List[dbWorkLine]:
searching/precomposedsqlsearching.py:def precomposedsqlwithinxlinessearch(so: SearchObject) -> List[dbWorkLine]:
searching/precomposedsqlsearching.py:def precomposedsqlwithinxwords(so: SearchObject) -> List[dbWorkLine]:
searching/precomposedsqlsearching.py:def precomposedsqlphrasesearch(so: SearchObject) -> List[dbWorkLine]:
searching/precomposedsqlsearching.py:def precomposedsqlsubqueryphrasesearch(so: SearchObject) -> List[dbWorkLine]:
searching/precomposedsqlsearching.py:def precomposedsqlsearchmanager(so: SearchObject) -> List[dbWorkLine]:
searching/precomposedsqlsearching.py:def workonprecomposedsqlsearch(workerid: int, foundlineobjects: ListProxy, listofplacestosearch: ListProxy,
searching/precomposedsqlsearching.py:def precomposedsqlsearcher(querydict, dbcursor) -> Generator:
searching/precomposesql.py:def searchlistintosqldict(searchobject: SearchObject, seeking: str, subqueryphrasesearch=False) -> dict:
searching/precomposesql.py:def rewritesqlsearchdictforlemmata(so: SearchObject) -> dict:
searching/precomposesql.py:def perparesoforsecondsqldict(so: SearchObject, initialhitlines: List[dbWorkLine], usebetweensyntax=True) -> SearchObject:
searching/precomposesql.py:def rewritequerystringforsubqueryphrasesearching(authortable: str, whereclause: str, so: SearchObject) -> str:
searching/precomposesql.py:def rewritesqlsearchdictforgolang(so: SearchObject) -> dict:
searching/searchhelperfunctions.py:def cleaninitialquery(seeking: str) -> str:
searching/searchhelperfunctions.py:def massagesearchtermsforwhitespace(query: str) -> str:
searching/searchhelperfunctions.py:def atsignwhereclauses(uidwithatsign, operand, authors) -> List[tuple]:
searching/searchhelperfunctions.py:def buildbetweenwhereextension(authortable: str, searchobject: SearchObject) -> str:
searching/searchhelperfunctions.py:def lookoutsideoftheline(linenumber: int, numberofextrawords: int, workid: str, searchobject: SearchObject, cursor) -> str:
searching/searchhelperfunctions.py:def findleastcommonterm(searchphrase: str, accentsneeded: bool) -> str:
searching/searchhelperfunctions.py:def findleastcommontermcount(searchphrase: str, accentsneeded: bool) -> int:
searching/searchhelperfunctions.py:def dblooknear(index: int, distanceinlines: int, secondterm: str, workid: str, usecolumn: str, dbcursor) -> bool:
searching/searchhelperfunctions.py:def buildsearchobject(searchid: str, therequest: request, thesession: session) -> SearchObject:
searching/searchhelperfunctions.py:def loadsearchqueue(iterable, workers):
searching/searchhelperfunctions.py:def rebuildsearchobjectviasearchorder(so: SearchObject) -> SearchObject:
searching/searchhelperfunctions.py:def grableadingandlagging(hitline: dbWorkLine, searchobject: SearchObject, cursor) -> dict:
searching/searchhelperfunctions.py:def redishitintodbworkline(redisresult: JSONDICT) -> dbWorkLine:
searching/searchhelperfunctions.py:def formatgolanggrabberarguments(command: str, so: SearchObject) -> list:
semanticvectors/gensimanalogies.py:def generateanalogies(sentencetuples, workssearched, searchobject, vectorspace) -> JSON_STR:
semanticvectors/gensimlsi.py:def lsigenerateoutput(sentencestuples, workssearched, searchobject, lsispace):
semanticvectors/gensimlsi.py:def lsifindmatches(sentencestuples, searchobject, lsispace):
semanticvectors/gensimlsi.py:def lsibuildspace(searchobject, morphdict, sentences):
semanticvectors/gensimmodels.py:def buildgensimmodel(searchobject, morphdict, sentences) -> Word2Vec:
semanticvectors/gensimnearestneighbors.py:def generatenearestneighbordata(sentencetuples, workssearched, searchobject, vectorspace):
semanticvectors/gensimnearestneighbors.py:def buildnnvectorspace(sentencetuples, searchobject):
semanticvectors/gensimnearestneighbors.py:def findapproximatenearestneighbors(query, mymodel, vectorvalues: VectorValues):
semanticvectors/gensimnearestneighbors.py:def findword2vecsimilarities(termone, termtwo, mymodel):
semanticvectors/gensimnearestneighbors.py:def trimbypartofspeech(listofwords: List[str], partofspeech: str, baggingmethod: str) -> set:
semanticvectors/gensimvectors.py:def executenearestneighborsquery(searchobject):
semanticvectors/gensimvectors.py:def executegenerateanalogies(searchobject):
semanticvectors/gensimvectors.py:def executegensimlsi(searchobject):
semanticvectors/gensimvectors.py:def executegensimsearch(searchobject, outputfunction, indextype):
semanticvectors/gensimvectors.py:def twodimensionalrepresentationofspace(searchobject):
semanticvectors/gensimvectors.py:def threedimensionalrepresentationofspace(searchobject):
semanticvectors/preparetextforvectorization.py:def vectorprepdispatcher(searchobject):
semanticvectors/preparetextforvectorization.py:def breaktextsintosentences(foundsentences, searchlist, searchobject, dbconnection):
semanticvectors/preparetextforvectorization.py:def monobreaktextsintosentences(searchlist, searchobject):
semanticvectors/rudimentaryvectormath.py:def finddotproduct(listofavalues, listofbvalues):
semanticvectors/rudimentaryvectormath.py:def findvectorlength(listofvalues):
semanticvectors/rudimentaryvectormath.py:def findcosinedist(avalues, lemmavalues, lemmavaluelength):
semanticvectors/rudimentaryvectormath.py:def caclulatecosinevalues(focusword, vectorspace, headwords):
semanticvectors/rudimentaryvectormath.py:def buildrudimentaryvectorspace(allheadwords, morphdict, sentences, subtractterm=None):
semanticvectors/rudimentaryvectormath.py:def vectorcosinedispatching(focusword, vectorspace, headwords):
semanticvectors/rudimentaryvectormath.py:def vectorcosineworker(headwords, workpiledict, resultdict):
semanticvectors/scikitlearntopics.py:def sklearnselectedworks(searchobject):
semanticvectors/scikitlearntopics.py:def ldatopicgraphing(sentencetuples, workssearched, searchobject, headwordstops=True):
semanticvectors/vectorgraphing.py:def graphbliteraldistancematches(searchterm, mostsimilartuples, searchobject):
semanticvectors/vectorgraphing.py:def graphnnmatches(searchterm, mostsimilartuples, vectorspace, searchobject):
semanticvectors/vectorgraphing.py:def graphmatches(graphtitle, searchterm, searchobject, mostsimilartuples, terms, relevantconnections, vtype):
semanticvectors/vectorgraphing.py:def matplotgraphmatches(graphtitle, searchterm, searchobject, mostsimilartuples, terms, relevantconnections, vtype):
semanticvectors/vectorgraphing.py:def storevectorgraph(figureasbytes):
semanticvectors/vectorgraphing.py:def fetchvectorgraph(imagename) -> bytes:
semanticvectors/vectorgraphing.py:def givetitletograph(topic, searchterm, searchobject):
semanticvectors/vectorgraphing.py:def generatethefineprint(vtype: str, vectorvalues: VectorValues) -> str:
semanticvectors/vectorgraphing.py:def tsnegraphofvectors(sentencetuples, workssearched, so, vectorspace):
semanticvectors/vectorgraphing.py:def threejsgraphofvectors(sentencetuples, workssearched, so, vectorspace):
semanticvectors/vectorgraphing.py:def reducetothreedimensions(so, vectorspace):
semanticvectors/vectorgraphing.py:def generategraphitdata(model, vectors, indices, clusters):
semanticvectors/vectorgraphing.py:def threedimensionaljs(jsgraphdata: str) -> str:
semanticvectors/vectorgraphing.py:def trheedimensionalhtml() -> str:
semanticvectors/vectorhelpers.py:def cleantext(texttostrip):
semanticvectors/vectorhelpers.py:def replaceabbreviations(foundstring, searchdict):
semanticvectors/vectorhelpers.py:def recursivesplit(tosplit, listofsplitlerms):
semanticvectors/vectorhelpers.py:def findsentences(authortable, searchobject, cursor):
semanticvectors/vectorhelpers.py:def parsevectorsentences(searchobject, lineobjects):
semanticvectors/vectorhelpers.py:def convertmophdicttodict(morphdict: dict) -> dict:
semanticvectors/vectorhelpers.py:def buildlemmatizesearchphrase(phrase: str) -> str:
semanticvectors/vectorhelpers.py:def bruteforcefinddblinefromsentence(thissentence, modifiedsearchobject):
semanticvectors/vectorhelpers.py:def finddblinesfromsentences(thissentence, sentencestuples, cursor):
semanticvectors/vectorhelpers.py:def convertlineuidstolineobject(listoflines, cursor):
semanticvectors/vectorhelpers.py:def convertsingleuidtodblineobject(lineuid, cursor):
semanticvectors/vectorhelpers.py:def mostcommoninflectedforms(cheat=True) -> set:
semanticvectors/vectorhelpers.py:def uselessforeignwords() -> set:
semanticvectors/vectorhelpers.py:def mostcommonheadwords(cheat=True) -> set:
semanticvectors/vectorhelpers.py:def mostcommonwordsviaheadwords() -> set:
semanticvectors/vectorhelpers.py:def removestopwords(sentencestring, stopwords):
semanticvectors/vectorhelpers.py:def relativehomonymnweight(worda, wordb, morphdict) -> float:
semanticvectors/vectorhelpers.py:def reducetotwodimensions(model) -> dict:
semanticvectors/vectorroutehelperfunctions.py:def findabsolutevectorsbysentence(searchobject):
semanticvectors/vectorroutehelperfunctions.py:def findabsolutevectorsfromhits(searchobject, hitdict, workssearched):
semanticvectors/vectorroutehelperfunctions.py:def generateabsolutevectorsoutput(listsofwords: list, workssearched: list, searchobject, vtype: str):
semanticvectors/vectorroutehelperfunctions.py:def emptyvectoroutput(searchobject, reasons=None):
semanticvectors/wordbaggers.py:def buildwordbags(searchobject, morphdict: dict, sentences: list) -> deque:
semanticvectors/wordbaggers.py:def buildwinnertakesallbagsofwords(morphdict, sentences) -> deque:
semanticvectors/wordbaggers.py:def buidunlemmatizedbagsofwords(morphdict, sentences) -> deque:
semanticvectors/wordbaggers.py:def buildflatbagsofwords(morphdict, sentences) -> deque:
semanticvectors/wordbaggers.py:def buildbagsofwordswithalternates(morphdict, sentences) -> deque:
textsandindices/indexmaker.py:def buildindextowork(cdict: dict, activepoll, headwords: bool, cursor) -> List[tuple]:
textsandindices/indexmaker.py:def generatesortedoutputbyword(completeindexdict: dict, onework: bool, alphabetical: bool) -> List[tuple]:
textsandindices/indexmaker.py:def generatesortedoutputbyheadword(completeindexdict: dict, onework: bool, alphabetical: bool, activepoll) -> List[tuple]:
textsandindices/indexmaker.py:def findindexbaseforms(completeindexdict, morphobjects, activepoll) -> dict:
textsandindices/indexmaker.py:def generateheadwordindexdict(augmentedindexdict) -> dict:
textsandindices/indexmaker.py:def htmlifysimpleindex(completeindexdict, onework) -> List[tuple]:
textsandindices/indexmaker.py:def linesintoindex(lineobjects: List[dbWorkLine], activepoll) -> dict:
textsandindices/indexmaker.py:def pooledindexmaker(lineobjects: List[dbWorkLine]) -> dict:
textsandindices/textandindiceshelperfunctions.py:def textsegmentfindstartandstop(authorobject, workobject, passageaslist, cursor) -> dict:
textsandindices/textandindiceshelperfunctions.py:def wordindextohtmltable(indexingoutput: List[tuple], useheadwords: bool) -> str:
textsandindices/textandindiceshelperfunctions.py:def dictmerger(masterdict: dict, targetdict: dict):
textsandindices/textandindiceshelperfunctions.py:def setcontinuationvalue(thisline: dbWorkLine, previousline: dbWorkLine, previouseditorialcontinuationvalue: bool, brktype: str, openfinder=None, closefinder=None):
textsandindices/textandindiceshelperfunctions.py:def getrequiredmorphobjects(setofterms: set, furtherdeabbreviate=False):
textsandindices/textandindiceshelperfunctions.py:def mpmorphology(terms: list, furtherdeabbreviate: bool, morphobjects, dbconnection: ConnectionObject):
textsandindices/textandindiceshelperfunctions.py:def paragraphformatting(listoflines: List[dbWorkLine]) -> List[dbWorkLine]:
textsandindices/textandindiceshelperfunctions.py:def spanopenedbutnotclosed(linehtml, openfinder, closefinder) -> str:
textsandindices/textandindiceshelperfunctions.py:def spanclosedbeforeopened(linehtml, openfinder, closefinder) -> bool:
textsandindices/textandindiceshelperfunctions.py:def oldparagraphformatting(listoflines: List[dbWorkLine]) -> List[dbWorkLine]:
textsandindices/textbuilder.py:def buildtext(work: str, firstline: int, lastline: int, linesevery: int, cursor) -> str:
textsandindices/textbuilder.py:def determinelinetemplate(shownotes=True) -> str:
threading/mpthreadcount.py:def setthreadcount(startup=False) -> int:
threading/vectordbautopilot.py:def startvectorizing():
threading/vectordbautopilot.py:def determinevectorworkpile(tempcap=False) -> List[tuple]:
threading/vectordbautopilot.py:def buildfakesearchobject(qtype='nearestneighborsquery') -> SearchObject:
threading/websocketthread.py:def startwspolling(theport=hipparchia.config['PROGRESSPOLLDEFAULTPORT']):